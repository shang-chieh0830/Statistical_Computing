[["index.html", "Statistical Computing Chapter 1 About", " Statistical Computing Shang-Chieh 2023-04-15 Chapter 1 About This book is a concise lecture note about Statistical Computing. The content of this book is from the course Statistical Computing taught by Jochen Voss. You can check his YouTube channel to get full(and correct) information about this course. Again, I do NOT own the content of this book. I write this book only for studying. All credits belong to Jochen Voss. If there is any copyright concerns, I will make this book private ASAP. "],["random-number-generation.html", "Chapter 2 Random number generation 2.1 Pseudo random number generators 2.2 The inverse transform method 2.3 Rejection sampling", " Chapter 2 Random number generation 2.1 Pseudo random number generators Definition 2.1 PRNG A PRNG is an algorithm which outputs a sequence of numbers, which can be used as a replacement for an i.i.d. sequence of random numbers. 2.1.1 Structure of PRNG in computer States: \\(s_0\\xrightarrow{f} s_1 \\xrightarrow{f} s_2 \\xrightarrow{f} s_3 \\xrightarrow{f} \\dots\\) Each state gives a random number : \\[s_1\\xrightarrow{g}X_1\\\\ s_2\\xrightarrow{g}X_2\\\\ s_3\\xrightarrow{g}X_3\\\\ \\vdots\\] where \\(s_i\\in S.\\) Here, \\(S\\) is called the state space, \\(f:S\\to S\\) is called the mixing function and \\(g:S\\to \\mathcal R\\) is called the output function. Note that \\(s_0\\) is never computed, \\(s_0\\) is called the seed. Linear congruential generator (LCG) \\(mod \\quad m\\) means that we divide by m and take the remainder(對m取餘) \\(X_n=a(X_{n-1}+c)\\quad mod \\quad m,\\) where \\(X_n \\in \\{0,1,...,m-1\\}, a\\in \\{1,2,...,m-1\\}\\), a is called the multiplier; \\(c\\in \\{0,1,...,m-1\\}\\), c is called the increment; \\(m\\in \\mathcal N=\\{1,2,3,...\\}\\), m is called the modulus. Under LCG, there’s no difference between X and s. That is, \\(X_n\\in\\{0,1,...,m-1\\}:=S,\\quad g(s)=s, \\quad f(X)=(aX+c)\\quad mod\\quad m\\) \\(X_0\\in S\\) is the seed in LCG. Example 2.1 LCG with \\(m=8, a=5, c=1, X_0=0\\) \\(X_1=(aX_0+c)\\quad mod\\quad m =(5\\times 0+1)\\quad mod\\quad8=1\\) \\(X_2=(aX_1+c)\\quad mod\\quad m =(5\\times 1+1)\\quad mod\\quad8=6\\) \\(X_3=(aX_2+c)\\quad mod\\quad m =(5\\times 6+1)\\quad mod\\quad8=7\\) m &lt;- 8 a &lt;- 5 c &lt;- 1 X0 &lt;- 0 Xi &lt;- X0 for (i in 1:10){ Xi &lt;- (a*Xi + c)%%m cat(&quot;X_&quot;, i, &quot; = &quot;, Xi, &quot;\\n&quot;, sep=&quot;&quot;) } ## X_1 = 1 ## X_2 = 6 ## X_3 = 7 ## X_4 = 4 ## X_5 = 5 ## X_6 = 2 ## X_7 = 3 ## X_8 = 0 ## X_9 = 1 ## X_10 = 6 rLCG &lt;- function(n, m, a, c, X0){ Xi &lt;- X0 for (i in 1:n){ Xi &lt;- (a*Xi + c)%%m cat(&quot;X_&quot;, i, &quot; = &quot;, Xi, &quot;\\n&quot;, sep=&quot;&quot;) } } rLCG(5,8,5,1,0) ## X_1 = 1 ## X_2 = 6 ## X_3 = 7 ## X_4 = 4 ## X_5 = 5 rLCG &lt;- function(n, m, a, c, X0){ result &lt;- numeric(n) Xi &lt;- X0 for (i in 1:n){ Xi &lt;- (a*Xi + c)%%m result[i] &lt;- Xi } return(result) } rLCG(5,8,5,1,0) ## [1] 1 6 7 4 5 rLCG(16,8,5,1,0) ## [1] 1 6 7 4 5 2 3 0 1 6 7 4 5 2 3 0 # Did you notice that it just repeat itself? # This will be discuss later with the quality of PRNG # also notice m=8, so it repeats itself every 8 numbers X &lt;- rLCG(5,8,5,1,0) plot(X) # the defult breaks on integer, which is not good X &lt;- rLCG(100000, 8,5,1,0) hist(X) X &lt;- rLCG(100000, 8,5,1,0) hist(X, breaks = seq(-0.5, 7.5, 1)) X &lt;- rLCG(10, 8,5,1,0) hist(X, breaks = seq(-0.5, 7.5, 1)) rLCG(10, m=2^31, a=1103515245, c=12345, X0=0) ## [1] 12345 1406932606 654583808 1358247936 2138638336 1459132416 ## [7] 1445521408 370866176 1896597568 1518859008 LCG is outdated nowadays, the purpose of introducing this method is to give you a general understanding of how pseudo random numbers are produced in computer. # the normal way to choose seed ?set.seed set.seed(0) runif(3) ## [1] 0.8966972 0.2655087 0.3721239 set.seed(1) runif(3) ## [1] 0.2655087 0.3721239 0.5728534 2.1.2 Quality of PRNGs There are a number of criteria. Period Length The computer has only finite memory, so we can’t have infinite state space. It turns out that PRNG starts repeating itself after a while (entering the states that it has already visited). The maximum period length is \\(|S|\\). For LCG, \\(S=\\{0,1,...,m-1\\} \\implies |S|=m\\) Note that random numbers never repeat, but pseudo random numbers DO!! But fortunately, we can have extremely large period in R. Good PRNG have large period. Distribution of samples Aim: generate uniformly distributed samples.\\(U(0,1)\\) For LCG, \\(X_n \\in \\{0,1,...,m-1\\}\\), and we’ve seen from previous examples that LCG is indeed uniformly distributed from \\(\\{0,1,...,m-1\\}\\), note that this is not \\(U(0,1)\\) but at least it’s uniformly distributed. To get \\(U(0,1)\\), use the transformation \\(U_n:=\\frac{X_n+1}{m+1}\\in (0,1)\\) for large m. You can use histogram or check whether the PRNG visits each state exactly once (which then produces every pseudo number once) within its maximum period length. Better criterion: PRNG output passes statistical tests for uniform distribution w/o problems (check example 1.6 in the book) Independence of samples You can just plot \\(X_n\\) and \\(X_{n-1}\\) on x-axis and y-axis respectively. X &lt;- rLCG(1000,8,5,1,0) # m=8 which means small state space, you won&#39;t expect this small m gives you uniformly distributed plot plot(X[1:999], X[2:1000]) X &lt;- rLCG(1000, m=2^32, a=1103515245, c=12345, X0=0) plot(X[1:999], X[2:1000]) # well, it now looks like uniformly distributed U &lt;- (X+1)/(2^32+1) range(U) ## [1] 2.874527e-06 9.979539e-01 round(range(U),4) ## [1] 0.000 0.998 plot(U[1:999], U[2:1000], asp=1) # Great, U(0,1) par(mai=c(0.9,0.9,0.1,0.1)) plot(U[1:999], U[2:1000], asp=1) # Use R&#39;s default PRNG: Mersenne-Twister X &lt;- runif(1000) plot(X[1:999], X[2:1000]) plot(X[1:999], X[2:1000], asp=1) # You can see there&#39;s no discernible pattern In fact, the states are not independent, each state is completely determined by previous state via the function f, you can check the algorithm again. So the states are definitely not independent. But what about those outputs \\(X_1,X_2,...\\) produced by the function g? Well, then you need to choose g properly such that g will lose some information from the states and thus make those outputs \\(X_1,X_2,...\\) independent. So theoretically, they are not independent unless you can choose g cleverly. From our scatter plots above, they actually look quite good. Definition 2.2 k-dimensionally equi-distributed A sequence of numbers is said to be k-dimensionally equidistributed if every k-tuple of possible outputs occurs equally often in this sequence. For example, consider LCG with k=2, we need to consider all pairs: (1,1), (1,2), (1,3),… They are 2 dimensionally equi-distributed if the frequency of all possible pairs converge to the same number. k-dimensionally equi-distributed is what we would expect for a sequence of independent random numbers. Good PRNG has this property for large k. Role of the seed Seed will determine which of the possible output sequences of PRNG we will get. set.seed(1) # always get the same output Reproducible output(for reports or debugging) For non-reproducible output: use techniques like current time of day as the seed. 2.2 The inverse transform method Aim: Convert \\(U\\sim U(0,1)\\) into X from a given distribution on \\(\\mathcal R\\) Definition 2.3 The cumulative distribution function (cdf) is given by \\[F(a)=P(X \\le a)\\] Moreover, \\[P(X\\in (a,b])=P(X \\le b)-P(X\\le a)=F(b)-F(a)\\] Proposition 2.1 \\[U \\sim U(0,1), X:=F^{-1}(u)\\\\ \\implies X \\text{ has cdf } F\\] Proof. \\(F^{-1}(u):=inf\\{x\\in \\mathcal R|F(x)\\ge u \\}\\) Let \\(U\\sim U(0,1), X=F^{-1}(u)\\). Then \\[P(X \\le a)=P(F^{-1}(u)\\le a)\\\\ =P(inf\\{x\\in \\mathcal R | F(x)\\ge u\\}\\le a)\\\\ =P(F(a)\\ge u)\\\\ =P(u\\le F(a))\\\\ =\\frac{F(a)-0}{1-0}\\\\ =F(a)\\] We use the facts that \\(inf\\{x\\in \\mathcal R | F(x)\\ge u\\}\\le a\\iff F(a)\\ge u\\) and \\(U\\sim U(0,1)\\) Example 2.2 Consider \\(X\\sim Exp(\\lambda)\\) \\(f(x)=\\begin{cases} \\lambda e^{-\\lambda x} &amp; x\\ge 0 \\\\ 0 &amp; x&lt;0 \\end{cases}\\) \\(\\implies F(a)=P(X\\le a)=\\int_{-\\infty}^{a}f(x)dx=\\int_{0}^{a}\\lambda e^{-\\lambda x}dx\\\\=(-e^{-\\lambda x})|_{x=0}^{a}=-e^{-\\lambda a}-(-e^{-\\lambda 0})=1-e^{-\\lambda a}\\) \\(u=F(x)=1-e^{-\\lambda x}\\\\ \\iff e^{-\\lambda x}=1-u \\iff -\\lambda x=log(1-u)\\iff x=-\\frac{log(1-u)}{\\lambda}\\) N &lt;- 1e5 U &lt;- runif(N) lambda &lt;- 2 X &lt;- -log(1-U)/lambda par(mai=c(0.9, 0.9, 0.1, 0.1)) hist(X, main=NULL, prob=TRUE, breaks=50) x &lt;- seq(0, max(X), length.out=100) lines(x, dexp(x, lambda), col=&quot;red&quot;) 2.3 Rejection sampling 2.3.1 Basic Rejection Sampling For n=1,2,3,…: Generate \\(X_n \\sim g\\) ( g is the proposal distribution) Generate \\(U_n\\sim U(0,1)\\) \\[\\begin{cases}output\\quad X_n (accept\\quad X_n), &amp; \\text{if } U_n\\le p=p(X_n)\\\\ ignore\\quad X_n(reject \\quad X_n), &amp; o.w.\\end{cases}\\] Note that \\(P(U\\le p)=p, \\forall p\\in[0,1]\\), so \\(P(U_n\\le p=p(X_n))=p=p(X_n)\\), i.e. we accept \\(X_n\\) with probability \\(p=p(X_n)\\). Here \\(p(X_n)\\) is to emphasize \\(p\\) depends on \\(X_n\\), \\(p(X_n)\\) is not the pdf of \\(X_n\\) # first experiment about rejection sampling N &lt;- 10000 X &lt;- numeric(N) i &lt;- 1 while (i&lt;=N){ Xi &lt;- rnorm(1) X[i] &lt;- Xi i &lt;- i+1 } par(mai=c(0.9, 0.9, 0.1, 0.1)) hist(X, main=NULL, prob=TRUE, breaks = 50) x &lt;- seq(-4, 4, length.out=200) lines(x, dnorm(x), col=&quot;red&quot;, lwd=2) N &lt;- 10000 X &lt;- numeric(N) i &lt;- 1 while (i&lt;=N){ Xi &lt;- rnorm(1) U &lt;- runif(1) p &lt;- ifelse(Xi&lt;0, 0.5, 1) # scale down the negative part if(U&lt;=p){ X[i] &lt;- Xi i &lt;- i+1 } } par(mai=c(0.9, 0.9, 0.1, 0.1)) hist(X, main=NULL, prob=TRUE, breaks = 50) x &lt;- seq(-4, 4, length.out=200) lines(x, dnorm(x), col=&quot;red&quot;, lwd=2) Let \\(\\phi(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}\\) be the std. normal pdf. In the above codes, p &lt;- ifelse(Xi&lt;0, 0.5, 1) scale down the negative part by 1/2, i.e. the area under the histogram is now 1/4 (negative part) + 1/2 (positive part) = 3/4, which is no longer 1. p &lt;- ifelse(Xi&lt;0, 0.5, 1), you can think of this as \\(p(x)\\phi(x)=\\begin{cases} 1\\phi(x), &amp; \\text{if positive part}\\\\ \\frac{1}{2}\\phi(x), &amp; \\text{if negative part} \\end{cases}\\) But now, \\(p(x)\\phi(x)\\) is not a probability distribution, because the area under \\(p(x)\\phi(x)\\) is 3/4. To make this be a probability distribution again, we will divide 3/4, i.e. \\(\\frac{1}{3/4}p(x)\\phi(x)=\\frac{1}{z}p(x)g(x)\\), where \\(z=3/4, g(x)=\\phi(x)\\) N &lt;- 10000 X &lt;- numeric(N) i &lt;- 1 count &lt;- 0 while (i&lt;=N){ Xi &lt;- rnorm(1) count &lt;- count+1 U &lt;- runif(1) p &lt;- ifelse(Xi&lt;0, 0.5, 1) # scale down the negative part if(U&lt;=p){ X[i] &lt;- Xi i &lt;- i+1 } } # The number of required proposals we have # to get N accepted samples cat(&quot;count =&quot;, count, &quot;\\n&quot;) ## count = 13278 par(mai=c(0.9, 0.9, 0.1, 0.1)) hist(X, main=NULL, prob=TRUE, breaks = 50) x &lt;- seq(-4, 4, length.out=200) lines(x, ifelse(x&lt;0, 0.5, 1) * dnorm(x) * 4/3, col=&quot;red&quot;, lwd=2) Proposition 2.2 Given proposals \\(X_n\\sim g\\), acceptance probability \\(p(x)\\in [0,1]\\) Let \\(N_k=\\) index of the kth accepted proposal (For example, \\(X_1, X_2, X_3, X_4, ...\\), say \\(X_3\\) is rejected, then \\(X_{N_1}=X_1, X_{N_2}=X_2, X_{N_3}=X_4,...\\)) Then the accepted samples have density \\[f(x)=\\frac{1}{z}p(x)g(x)\\], i.e. \\((X_{N_k})\\) is an i.i.d. sequence with density f. Each sample is accepted with probability \\(z\\). The number of required proposals is geometrically distributed with mean \\(\\frac{1}{z}\\). Note that \\(1=\\int_{\\mathcal R} \\frac{1}{z}p(x)g(x)dx=\\frac{1}{z}\\int_{\\mathcal R} p(x)g(x)dx\\\\ \\implies z=\\int p(x)g(x)dx\\) Proof. The probability of accepting \\(X_n\\) is \\[P(X_n accepted)=P(U_n\\le p(X_n))=\\\\ =\\int_{\\mathcal R}\\int_{0}^{1}\\boldsymbol 1_{\\{U\\le p(X)\\}} du\\quad g(x) dx\\\\ =\\int_{\\mathcal R}p(x)g(x)dx=z\\] Note that \\(p(x)=\\int_{0}^{1}\\boldsymbol 1_{\\{U\\le p(X)\\}} du\\) and \\(\\boldsymbol 1_{\\{U\\le p(X)\\}}=\\begin{cases}1, &amp; \\text{if } U_n\\le p(X_n)\\\\ 0, &amp; o.w. \\end{cases}\\) Now, we want to show \\(P(X_{N_k}\\in A)=\\int_{A}f(x)dx\\) \\[P(X_{N_k}\\in A|N_{k-1}=n)=\\sum_{m=1}^{\\infty}P(X_{N_k}\\in A, N_k=n+m|N_{k-1}=n)\\\\ =\\sum_{m=1}^{\\infty}P(X_{N_{n+m}}\\in A, N_k=n+m|N_{k-1}=n)\\\\ =\\sum_{m=1}^{\\infty}P(X_{n+m}\\in A, U_{n+1}&gt;p(X_{n+1}),...,\\\\ U_{n+m-1}&gt;p(X_{n+m-1}), U_{n+m}\\le p(X_{n+m})|N_{k-1}=n)\\\\ =\\sum_{m=1}^{\\infty}P(U_{n+1}&gt;p(X_{n+1}),..., U_{n+m-1}&gt;p(X_{n+m-1})\\\\, U_{n+m}\\le p(X_{n+m}), X_{n+m}\\in A|N_{k-1}=n)\\\\ =\\sum_{m=1}^{\\infty}P(U_{n+1}&gt;p(X_{n+1})|N_{k-1}=n)\\times \\dots \\times\\\\ P(U_{n+m-1}&gt;p(X_{n+m-1})|N_{k-1}=n)P(X_{n+m}\\in A, U_{n+m}\\le p(X_{n+m})|N_{k-1}=n)\\] The last equation come about b/c of independence. Now, observe: \\(P(U_{n+1}&gt;p(X_{n+1})|N_{k-1}=n)=1-z\\) \\(\\vdots\\) \\(P(U_{n+m-1}&gt;p(X_{n+m-1})|N_{k-1}=n)=1-z\\) \\(P(U_{n+m}\\le p(X_{n+m}), X_{n+m}\\in A|N_{k-1}=n)\\\\=\\int_{\\mathcal R}\\int_{1}^{0}\\boldsymbol 1_{\\{U\\le p(X), X\\in A\\}} du\\quad g(x)dx\\\\=\\int_{\\mathcal R}\\int_{1}^{0}\\boldsymbol 1_{\\{U\\le p(X)\\}}\\boldsymbol 1_{\\{ X\\in A\\}} du\\quad g(x)dx\\\\=\\int_{\\mathcal R}\\int_{1}^{0}\\boldsymbol 1_{\\{U\\le p(X)\\}}du\\quad \\boldsymbol 1_{\\{ X\\in A\\}}g(x)dx\\\\=\\int_{\\mathcal R}p(x)\\boldsymbol 1_{\\{ X\\in A\\}}g(x)dx\\\\=\\int_{A}p(x)g(x)dx\\\\\\) (One thing to notice: \\(\\boldsymbol 1_{\\{U\\le p(X), X\\in A\\}}=\\boldsymbol 1_{\\{U\\le p(X)\\}}\\boldsymbol 1_{\\{ X\\in A\\}}\\), This is because these two conditions must hold at the same time to get 1) Hence, \\[P(X_{N_k}\\in A|N_{k-1}=n)=\\sum_{m=1}^{\\infty}P(X_{N_k}\\in A, N_k=n+m|N_{k-1}=n)\\\\ =\\sum_{m=1}^{\\infty}P(U_{n+1}&gt;p(X_{n+1})|N_{k-1}=n)\\times \\dots \\times\\\\ P(U_{n+m-1}&gt;p(X_{n+m-1})|N_{k-1}=n)P(X_{n+m}\\in A, U_{n+m}\\le p(X_{n+m})|N_{k-1}=n)\\\\ = (\\sum_{m=1}^{\\infty}(1-z)^{m-1})\\int_{A}p(x)g(x)dx\\\\ =(\\sum_{m=0}^{\\infty}(1-z)^{m})\\int_{A}p(x)g(x)dx\\\\ =\\frac{1}{1-(1-z)}\\int_{A}p(x)g(x)dx\\\\ =\\int_{A}\\frac{1}{z}p(x)g(x)dx\\\\ =\\int_{A}f(x)dx\\], which is true independent of n Hence \\(P(X_{N_k}\\in A)=\\int_{A}f(x)dx\\), i.e. \\(X_{N_k}\\) has desity f. Recall the fact that \\(\\sum_{m=0}^{\\infty}a^m=\\frac{1}{1-a},\\forall |a|&lt;1\\) Example 2.3 Consider \\(X_n\\sim U(-1,+1)\\implies g(x)=\\boldsymbol 1_{[-1,+1]}\\times \\frac{1}{2}\\), and \\(p(x)=\\sqrt{1-x^2}\\), which is the upper half of the unit circle. Then \\(f(x)=\\frac{1}{z}p(x)g(x)=\\frac{1}{z}\\times \\begin{cases} \\sqrt{1-x^2}\\times \\frac{1}{2}, &amp; \\forall x\\in [-1,+1]\\\\ 0, &amp; \\forall x \\notin[-1,+1] \\end{cases}\\) So, \\(z=\\int_{-1}^{+1}p(x)g(x)dx=\\int_{-1}^{+1}\\sqrt{1-x^2}\\times 1/2dx=1/2\\int_{-1}^{+1}\\sqrt{1-x^2}dx\\\\=1/2\\times \\frac{1}{2}\\pi1^2\\\\=\\pi/4\\) This implies the accepted samples have density \\[f(x)=\\begin{cases}2/\\pi\\sqrt{1-x^2} &amp; \\forall x\\in [-1,+1]\\\\0 &amp; o.w. \\end{cases}\\] In fact, this distribution f(x) is the so-called Wigner semicircle distribution. N &lt;- 10000 X &lt;- numeric(N) i &lt;- 1 count &lt;- 0 while (i&lt;=N){ Xi &lt;- runif(1,-1,1) count &lt;- count+1 U &lt;- runif(1) p &lt;- sqrt(1-Xi^2) if(U&lt;=p){ X[i] &lt;- Xi i &lt;- i+1 } } # The number of required proposals we have # to get N accepted samples cat(&quot;count =&quot;, count, &quot;\\n&quot;) ## count = 12732 par(mai=c(0.9, 0.9, 0.1, 0.1)) hist(X, main=NULL, prob=TRUE,asp=1) x &lt;- seq(-1, 1, length.out= 100) f.of.x &lt;- 4/pi * sqrt(1-x^2) * 1/2 lines(x, f.of.x, col=&quot;red&quot;, lwd=2) z=pi/4 # The number of required proposals is geometrically distributed # with mean 1/z 1/z ## [1] 1.27324 2.3.2 Envelope Rejection Sampling Recall: In previous sections, we generate \\(X\\sim g\\) and \\(U\\sim U(0,1)\\). We accept X, if \\(U\\le p(X)\\). Also, accepted sample have density \\(f(x)=\\frac{1}{z}p(x)g(x)\\). Note that this implies \\(p(x)=constant\\times\\frac{f(x)}{g(x)}.\\) But \\(p(x)\\) is a probability, so we must have \\(\\frac{f(x)}{g(x)}\\) to be bounded, i.e. \\(f(x)\\le c\\times g(x),\\forall x\\) for some constant c, so that we can adjust the constant term in \\(p(x)=constant\\times\\frac{f(x)}{g(x)}\\) such that \\(p(x)\\in [0,1].\\) It turns out this constant term is \\(\\frac{1}{c}\\), i.e. \\(p(x)=\\frac{1}{c}\\frac{f(x)}{g(x)}\\) In this section, instead of finding what \\(f\\) is, \\(f\\) is already GIVEN, envelope rejection sampling can do this. \\(f(x)\\le c\\times g(x),\\forall x\\), is why we call it envelope rejection sampling. We will only require f to known up to a multiplicative constant (e.g. instead of knowing \\(f(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}\\), we only need to know \\(f(x)=e^{-x^2/2}\\)), and we assume \\(f(x)\\ge 0\\) (don’t need to integrate to 1.), to get samples from \\[\\tilde f(x)=\\frac{1}{z_f}f(x), z_f=\\int_{\\mathcal R}f(x)dx\\] Proposition 2.3 Input: \\(f: \\mathcal R \\to [0,\\infty)\\) with \\(z_f=\\int f(x)dx&lt;\\infty\\), \\(g: \\mathcal R \\to [0,\\infty)\\) be a probability density, and a constant c such that \\(f(x)\\le cg(x)\\) Generate \\(X\\sim g\\) and \\(U\\sim U(0,1)\\). We accept X, if \\(cg(x)\\times U\\le f(x)\\) (you can write \\(U\\le \\frac{f(x)}{cg(x)}\\), but make sure that the denominator ≠ 0) Then we have: Accepted samples have density \\(\\tilde f(x)=\\frac{1}{z_f}f(x)\\) Each proposal is accepted with probability \\(\\frac{z_f}{c}\\), and mean number of proposals per output is \\(\\frac{c}{z_f}\\) Proof. Each proposal is accepted with probability \\(p(x)=\\begin{cases} \\frac{f(x)}{cg(x)} &amp; \\text{ if } g(x)&gt;0 \\\\ 1 &amp; \\text{ if } g(x)=0 \\end{cases}.\\) Note that when \\(g(x)=0\\), \\(cg(x)\\times U=0\\le f(x)\\) always holds b/c we also assume \\(f(x)&gt;0\\), so we must accept X when \\(g(x)=0\\) By basic rejection sampling, the accepted samples have density \\(\\frac{1}{z}p(x)g(x)=\\begin{cases} \\frac{1}{z}\\frac{f(x)}{cg(x)}g(x)=f(x)/zc &amp; \\text{ if } g(x)&gt;0\\\\ \\frac{1}{z}\\times 1\\times g(x)=0 &amp; \\text{ if }g(x)=0\\end{cases}\\) In fact, from the above we get \\(\\frac{1}{z}p(x)g(x)=\\frac{f(x)}{zc}\\) no matter whether \\(g(x)=0\\). This is b/c \\(f(x)\\le c\\times g(x),\\forall x\\) so \\(g(x)=0\\implies f(x)=0\\) \\(z=\\int p(x)g(x)dx=\\frac{1}{c}\\int f(x)dx=\\frac{z_f}{c}\\\\ \\implies\\frac{1}{z}p(x)g(x)=\\frac{c}{z_f}\\frac{f(x)}{c}=\\frac{1}{z_f}f(x)=\\tilde f(x)\\) Each sample is accepted with probability \\(z=z_f/c\\) Example 2.4 Consider \\(f(x)=e^{-x^2/2}\\), we want to find some constant c and proposal \\(g(x)\\) so that \\(f(x)\\le cg(x)\\) Since \\(f(x)\\) is pretty similar to std. Normal, one can consider \\(g(x)\\) to be double exponential distribution. For proposals: \\(X\\sim \\frac{\\lambda}{2}e^{-\\lambda|x|}=:g(x)\\) (Let’s digress a little bit, how can we use inverse transform sampling to generate random samples from double exponential distribution? Steps: Generate \\(U\\sim U(0,1)\\), Generate \\(Y=-\\frac{log(1-U)}{\\lambda}\\) (note that this is from exponential distribution, so it only has positive side of double exponential), Generate \\(V\\sim U(0,1)\\) (double exponential is symmetric to 0), and let \\(X=\\begin{cases}Y, &amp; V\\le1/2 \\\\-Y, &amp; V&gt;1/2 \\end{cases}\\) ) Now, can we find c with \\(f(x)\\le cg(x), \\forall x\\in \\mathcal R\\), or equivalently, \\(f(x)/g(x)\\le c, \\forall x\\in \\mathcal R\\)? (The equivalence holds b/c g(x) is never 0 in this example) \\(\\frac{f(x)}{g(x)}=\\frac{e^{-x^2/2}}{\\frac{\\lambda}{2}e^{-\\lambda |x|}}=\\frac{2}{\\lambda}e^{-\\frac{x^2}{2}+\\lambda |x|}\\) Note that double exponential distribution is symmetric to 0, so we can simply consider the positive part (\\(x\\ge 0\\)), so now we have \\(\\frac{f(x)}{g(x)}=\\frac{2}{\\lambda}e^{-\\frac{x^2}{2}+\\lambda |x|}=\\frac{2}{\\lambda}e^{-\\frac{x^2}{2}+\\lambda x}\\) Also note that by F.O.C, we have \\(0=(-\\frac{x^2}{2}+\\lambda x)&#39;=-x+\\lambda \\iff x=\\lambda\\), i.e. \\(-\\frac{x^2}{2}+\\lambda x\\) has max at \\(x=\\lambda\\) This gives \\(\\frac{f(x)}{g(x)}=\\frac{2}{\\lambda}e^{-\\frac{x^2}{2}+\\lambda |x|}=\\frac{2}{\\lambda}e^{-\\frac{x^2}{2}+\\lambda x}\\le \\frac{2}{\\lambda}e^{-\\frac{\\lambda^2}{2}+\\lambda^2}=\\frac{2}{\\lambda}e^{\\frac{\\lambda^2}{2}}=:c\\) So yes, we found c with \\(f(x)/g(x)\\le c, \\forall x\\in \\mathcal R\\) Now, following the steps in envelope rejection sampling, we then: Generate \\(W\\sim U(0,1)\\), and accept X if \\(cg(x)\\times W\\le f(x) \\\\ \\implies \\frac{2}{\\lambda}e^{\\frac{\\lambda^2}{2}}\\frac{\\lambda}{2}e^{-\\lambda|x|}W\\le e^{-x^2/2}\\\\ \\implies W\\le exp(-\\frac{x^2}{2}+\\lambda |x|-\\frac{\\lambda^2}{2})\\) # converting double-exponential to normal distribution # using rejection sampling lambda &lt;- 1 #non-normalized target distribution f &lt;- function(x) exp(-x^2/2) # proposal density g &lt;- function(x) lambda/2 * exp(-lambda*abs(x)) # const c for envelope sampling c &lt;- 2/lambda * exp(lambda^2/2) x &lt;- seq(-3,3,length.out=200) plot(x, c*g(x), type=&quot;l&quot;, col=&quot;blue&quot;) lines(x, f(x)) N &lt;- 1e5 U &lt;- runif(N) Y &lt;- -log(1-U)/lambda V &lt;- runif(N) X &lt;- ifelse(V&lt;=1/2, Y, -Y) par(mai=c(0.9,0.9,0.2,0.1)) hist(X, prob=TRUE, breaks=50, main=NULL) x &lt;- seq(-10,10, length.out=200) lines(x, g(x), col=&quot;red&quot;, lwd=2) N &lt;- 1e5 V &lt;- rexp(N, lambda) X &lt;- ifelse(V&lt;=1/2, Y, -Y) par(mai=c(0.9,0.9,0.2,0.1)) hist(X, prob=TRUE, breaks=50, main=NULL) x &lt;- seq(-10,10, length.out=200) lines(x, g(x), col=&quot;red&quot;, lwd=2) N &lt;- 1e5 X &lt;- numeric(N) i &lt;- 1 count &lt;- 0 while (i&lt;=N){ U &lt;- runif(1) Y &lt;- -log(1-U)/ lambda V &lt;- runif(1) Xi &lt;- ifelse(V&lt;=1/2, Y, -Y) count &lt;- count+1 W &lt;- runif(1) if(c*g(Xi)*W &lt;= f(Xi)){ X[i] &lt;- Xi i &lt;- i+1 } } cat(&quot;count =&quot;, count, &quot;\\n&quot;) ## count = 131439 par(mai=c(0.9,0.9,0.2,0.1)) hist(X, prob=TRUE, breaks=50, main=NULL) x &lt;- seq(-4,4, length.out=200) lines(x, dnorm(x), col=&quot;red&quot;, lwd=2) 2.3.3 Choice of g We must be able to efficiently generate samples from g We must have \\(f(x)\\le cg(x), \\forall x\\). In particular, g must have heavier tails than f, i.e. we cannot have \\[\\lim_{|x|\\to \\infty}\\frac{g(x)}{f(x)}=0\\], i.e. \\(g(x)\\to 0\\) faster than \\(f(x)\\to 0\\) Example 2.5 \\(f(x)\\sim e^{-x^2/2}, g\\sim e^{-\\lambda|x|}\\), note that \\(e^{-x^2/2}\\) decays faster than \\(e^{-\\lambda|x|}\\) as \\(|x|\\to \\infty\\) so this example works fine. \\(f(x)\\sim \\frac{1}{x^\\beta}\\) (for large x), \\(g\\sim e^{-\\lambda x}\\), note that \\(\\frac{1}{x^\\beta}\\) decays slower \\(\\implies\\) cannot find c, so this example doesn’t work. 2.3.4 Efficiency On average, we need \\(\\frac{c}{z_f}\\) proposals to generate one sample. We should minimize c for given f and g !! g should have a similar shape to f. "],["simulating-statistical-models.html", "Chapter 3 Simulating Statistical Models 3.1 Markov Chains", " Chapter 3 Simulating Statistical Models 3.1 Markov Chains Definition 3.1 A stochastic process \\((X_t)_{t\\in \\mathcal N_0}\\) with values \\(X_t\\in S\\) is called a Markov Chain with state space \\(S\\) if \\[P(X_{t+1}\\in A_{t+1}| X_t=a_t,...,X_0=a_0)=P(X_{t+1}\\in A_{t+1}|X_t=a_t)\\\\ \\forall a_0,...,a_t\\in S, A_{t+1}\\subseteq S, t\\in \\mathcal N_0\\] where \\(\\mathcal N_0=\\mathcal N\\cup \\{0\\}\\) i.e. knowing Just Before is enough. Example 3.1 Random Walk Let \\((\\epsilon_n)_{n\\in \\mathcal N}\\) be a sequence of i.i.d. random variables, and let \\[X_n:=\\sum_{i=1}^{n}\\epsilon_i, \\forall n\\in \\mathcal N\\\\ X_0:=0\\] Then \\((X_n)_{n\\in \\mathcal N_0}\\) is a Markov Chain (MC). To see this, note that \\(X_{n+1}=\\sum_{i=1}^{n+1}\\epsilon_i=\\sum_{i=1}^{n}\\epsilon_i+\\epsilon_{n+1}=X_n+\\epsilon_{n+1}\\), \\(X_n\\) \\(\\perp\\) \\(\\epsilon_{n+1}\\) So \\(X_{n+1}\\) only depends on \\(X_n\\) Example 3.2 Let \\((\\epsilon_n)_{n\\in \\mathcal N}\\) be i.i.d., and let \\[X_0=0, X_1=0,\\\\ X_n=\\frac{X_{n-1}+X_{n-2}}{2}+\\epsilon_n, \\forall n\\in \\{2,3,...\\}\\] \\(X_{n+1}=\\frac12 X_n+\\frac12 X_{n-1}+\\epsilon_{n+1}\\) \\(X_{n+1}\\) not only depends on \\(X_n\\) but also \\(X_{n-1}\\) so not a MC. A MC is described by the probability \\(p_{xy}(t)=P(X_{t+1}=y|X_t=x)\\) the probability \\(\\pi_x=P(X_0=x)\\) (This just controls the initial condition, also note that this notation is only valid in discrete cases) If \\(p_{xy}(t)=p_{xy}\\) independent of t, then \\((X_t)_{t\\in \\mathcal N}\\) is called time homogeneous. From now on, we will assume time homogeneous. 3.1.1 Discrete case If S is discrete, then we can characterise transition probabilities by the probabilities \\(p_{xy}=P(X_{t+1}=y|X_t=x),\\forall t\\in \\mathcal N_0\\) If S is finite, we can write the \\(p_{xy}\\) as a matrix. Example 3.3 Consider \\(S=\\{1,2,3\\}\\) \\(P=\\begin{bmatrix}\\frac12 &amp; \\frac12 &amp; 0 \\\\ 0 &amp; \\frac12 &amp; \\frac12 \\\\ 1&amp; 0 &amp; 0 \\end{bmatrix}\\) \\(p_{xy}\\) corresponds to row x, column y \\(\\implies P(X_{t+1}=2|X_t=1)=p_{12}=\\frac12\\) Note that from this matrix, you can see when in state 3, you must go to state 1 in next step. P &lt;- matrix(c(1/2, 1/2, 0, 0, 1/2, 1/2, 1, 0, 0), 3, 3, byrow=TRUE) # For initial condition # remember there are 3 states pi &lt;- c(1/3, 1/3, 1/3) # sampling 1 number from 1:3 with prob=pi X0 &lt;- sample(3, 1, prob=pi) Xi &lt;- X0 for (i in 1:10){ p.next &lt;- P[Xi,] Xi &lt;- sample(3,1, prob=p.next) cat(&quot;X[&quot;, i, &quot;] = &quot;, Xi, &quot;\\n&quot;, sep=&quot;&quot;) } ## X[1] = 2 ## X[2] = 3 ## X[3] = 1 ## X[4] = 1 ## X[5] = 2 ## X[6] = 2 ## X[7] = 2 ## X[8] = 3 ## X[9] = 1 ## X[10] = 1 P &lt;- matrix(c(1/2, 1/2, 0, 0, 1/2, 1/2, 1, 0, 0), 3, 3, byrow=TRUE) # For initial condition # remember there are 3 states pi &lt;- c(1/3, 1/3, 1/3) # sampling 1 number from 1:3 with prob=pi X0 &lt;- sample(3, 1, prob=pi) n &lt;- 100 X &lt;- numeric(n) Xi &lt;- X0 for (i in 1:n){ p.next &lt;- P[Xi,] Xi &lt;- sample(3,1, prob=p.next) X[i] &lt;- Xi } X ## [1] 1 1 2 2 3 1 2 2 2 2 3 1 1 1 2 3 1 1 1 2 3 1 2 2 2 2 2 3 1 2 2 2 3 1 2 2 3 ## [38] 1 1 1 2 2 2 2 3 1 2 3 1 1 2 3 1 2 3 1 2 3 1 1 1 1 2 2 2 2 3 1 1 1 2 3 1 1 ## [75] 1 2 2 3 1 2 3 1 1 1 2 2 3 1 2 3 1 2 2 3 1 1 2 2 3 1 plot(X, type=&quot;b&quot;) hist(X, main = NULL) A transition matrix \\(P\\) satisfies \\(p_{xy}\\ge 0, \\forall x,y\\in S\\) \\(\\sum_{y\\in S}p_{xy}=1, \\forall x\\in S\\) (對row加總=1) An initial distribution \\(\\pi\\) satisfies \\(\\pi_x\\ge0,\\forall x\\in S\\) \\(\\sum_{x\\in S}\\pi_x=1\\) 3.1.2 Continuous case The notation \\(P(X_{t+1}=y|X_t=x)\\) is no longer valid.(單點機率為0) Instead, we consider \\(P(X_{t+1}\\in A|X_t=x)=\\int_{A}p(x,y)dy\\), where \\(p(x,y)\\) is called the transition density. Properties: \\(p(x,y)\\ge 0 ,\\forall x,y\\in S\\) \\(\\int p(x,y)dy=1, \\forall x\\in S\\) As for initial distribution, we have \\(P(X_0\\in A)=\\int_A\\pi(x)dx\\) and \\(\\pi(x)\\ge 0, \\forall x\\in S\\), \\(\\int \\pi(x)dx=1\\) 3.1.3 How the distribution of \\(X_t\\) changes as t changes? Assume that \\(P(X_t=x)=\\pi_x^{(t)}, \\forall x \\in S\\), (\\(\\pi^{(t)}\\) is the distribution of \\(X_t\\)) Question: What is \\(\\pi^{(t+1)}\\)? Ans: We have \\(\\pi^{(t+1)}_x=P(X_{t+1}=x)\\) (this is just notation) Note that \\(\\pi\\) is column vector. \\[\\pi_x^{(t+1)}=P(X_{t+1}=x)=\\sum_{y\\in S}P(X_{t+1}=x, X_t=y)\\\\ =\\sum_{y\\in S}P(X_{t+1}=x|X_t=y)P(X_t=y)\\\\ =\\sum_{y\\in S}p_{yx}\\pi_y^{(t)}\\\\ =(P^T \\pi^{(t)})_x, \\forall x \\in S\\\\ \\implies (\\pi^{(t+1)})^T=(\\pi^{(t)})^TP\\] \\(\\implies \\pi^{(t)}=(P^T)^t\\pi^{(0)}\\), with initial condition: \\(\\pi^{(0)}\\), i.e. multiply \\(P^T\\) t times. 3.1.4 Stationary Condition Assume that \\(P(X_t=x)=\\pi_x^{(t)}, \\forall x \\in S\\), (i.e. \\(\\pi^{(t)}\\) is the distribution of \\(X_t\\)) \\(\\pi\\) is a stationary distribution if \\(\\pi^{(t)}=\\pi\\implies \\pi^{(t+1)}=\\pi\\), or \\(P^T\\pi=\\pi\\), or equivalently \\(\\pi^TP=\\pi^T\\) How to find stationary distribution in computer? \\(P^T\\pi=1\\times \\pi\\iff \\pi\\) is an eigenvector of \\(P^T\\) with eigen value 1. P &lt;- matrix(c( 0.5, 0.5, 0.0, 0.0, 0.5, 0.5, 1.0, 0.0, 0.0), 3, 3, byrow = TRUE) e &lt;- eigen(t(P)) e ## eigen() decomposition ## $values ## [1] 1+0.0i 0+0.5i 0-0.5i ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.6666667+0i 0.7071068+0.0000000i 0.7071068+0.0000000i ## [2,] -0.6666667+0i -0.3535534-0.3535534i -0.3535534+0.3535534i ## [3,] -0.3333333+0i -0.3535534+0.3535534i -0.3535534-0.3535534i # extract the eigenvector with eigenvalue 1 e$vectors[,1] ## [1] -0.6666667+0i -0.6666667+0i -0.3333333+0i # pi.stationary = numbers in eigenvector pi.stat &lt;- c(2/3, 2/3, 1/3) pi.stat ## [1] 0.6666667 0.6666667 0.3333333 # Recall that eigenvectors are only defined # up to multiplicative constants # so you can multiply constants # we want the values sum to 1 # so we divide by sum(pi.stat) pi.stat &lt;- pi.stat/sum(pi.stat) pi.stat ## [1] 0.4 0.4 0.2 # check t(P) %*% pi.stat = pi.stat t(P)%*% pi.stat ## [,1] ## [1,] 0.4 ## [2,] 0.4 ## [3,] 0.2 P &lt;- matrix(c(1/2, 1/2, 0, 0, 1/2, 1/2, 1, 0, 0), 3, 3, byrow=TRUE) # stationary pi &lt;- c(0.4, 0.4, 0.2) # sampling 1 number from 1:3 with prob=pi X0 &lt;- sample(3, 1, prob=pi) n &lt;- 10000 X &lt;- numeric(n) Xi &lt;- X0 for (i in 1:n){ p.next &lt;- P[Xi,] Xi &lt;- sample(3,1, prob=p.next) X[i] &lt;- Xi } par(mai=c(0.9,0.9,0.1,0.1)) hist(X, breaks = c(0.5,1.5,2.5,3.5), main=NULL, probability = TRUE) P &lt;- matrix(c(1/2, 1/2, 0, 0, 1/2, 1/2, 1, 0, 0), 3, 3, byrow=TRUE) # pi &lt;- c(0.4, 0.4, 0.2) pi &lt;- c(1/3, 1/3, 1/3) # should converge to stationary # when n is large # sampling 1 number from 1:3 with prob=pi X0 &lt;- sample(3, 1, prob=pi) n &lt;- 10000 X &lt;- numeric(n) Xi &lt;- X0 for (i in 1:n){ p.next &lt;- P[Xi,] Xi &lt;- sample(3,1, prob=p.next) X[i] &lt;- Xi } par(mai=c(0.9,0.9,0.1,0.1)) hist(X, breaks = c(0.5,1.5,2.5,3.5), main=NULL, probability = TRUE) "],["monte-carlo-methods.html", "Chapter 4 Monte Carlo methods 4.1 Studying models via simulation 4.2 Monte Carlo estimates 4.3 Variance reduction methods", " Chapter 4 Monte Carlo methods 4.1 Studying models via simulation Recall: Definition 4.1 In discrete case, expectation is defined to be \\(E(X)=\\sum_x xP(X=x)\\) \\(E(f(X))=\\sum_x f(x)P(X=x)\\) In continuous case, we have \\(E(X)=\\int_{\\mathcal R}x\\phi(x)dx\\) \\(E(f(X))=\\int_{\\mathcal R}f(x)\\phi(x)dx\\) where \\(\\phi(x)\\) is the density of \\(X\\) at \\(x\\) \\(\\underline{\\text{Law of Large Numbers (LLN)}}\\) Let \\((X_j)_{j\\in \\mathcal N}\\) be a sequence of i.i.d. copies of \\(X\\) Then \\[\\lim_{N\\to \\infty}\\frac1N \\sum_{j=1}^{N}f(X_j)=E[f(X)]\\] Ideas for approximating \\(E[f(X)]\\): Use \\(E(X)=\\int x\\phi(x)dx\\) and try to numerically approximate the integral, this relates to numerical analysis and will not be covered here. \\(E(f(X))=\\lim_{N\\to \\infty}\\frac1N \\sum_{j=1}^{N}f(X_j)\\\\ \\approx \\frac1N \\sum_{j=1}^{N}f(X_j)\\) for a large N i.e., \\(Z^{MC}_N=\\frac1N\\sum_{j=1}^{N}f(X_j)\\), where \\(X_j\\), j=1,…,N are i.i.d. copies of \\(X\\), then \\(Z^{MC}_N\\approx E[f(X)]\\) Example 4.1 Assume \\(X\\sim N(\\mu, \\sigma^2)\\), assume we want to estimate \\(E[sin(X)]\\), i.e., \\(f(X)=sin(X),\\quad Z^{MC}_N=\\frac1N\\sum_{j=1}^{N}sin(X_j),\\quad X_1,...,X_N\\sim i.i.d. N(\\mu, \\sigma^2)\\) # aim: estimate E(sin(X)) where X is normal mu &lt;- 1 sigma &lt;- 1 N &lt;- 1000000 X &lt;- rnorm(N, mu, sigma) mean(sin(X)) ## [1] 0.5109993 Applications: computing probabilities Recall a indicator function: \\(\\boldsymbol 1_A(x)=\\begin{cases}1 &amp; \\text{ if }x\\in A\\\\ 0 &amp; \\text{ if }x\\notin A\\end{cases}\\) \\(\\implies E(\\boldsymbol 1_A(x))=0\\times P(\\boldsymbol 1_A(x)=0)+1\\times P(\\boldsymbol 1_A(x)=1)=P(X\\in A)\\\\ \\implies P(X\\in A)=E(\\boldsymbol 1_A(x))=\\frac1N\\sum_{j=1}^{N}\\boldsymbol 1_A(X_j)\\), which is exactly the proportion of \\(X_j\\) which are in A # aim: estimate P(X in A), where X is a random variable and A is a set. # assume X ~ N(0,1) # let A=[1, 2] N &lt;- 1e6 X &lt;- rnorm(N) f.of.X &lt;- ifelse(X&gt;=1 &amp; X&lt;=2, 1, 0) mean(f.of.X) ## [1] 0.136131 # In fact, we can get the exact result pnorm(2)-pnorm(1) ## [1] 0.1359051 computing integrals Assume we want to approximate \\(\\int_{a}^{b}h(x)dx\\) Choose \\(X\\sim U(a,b)\\implies \\phi(x)=\\begin{cases}\\frac{1}{b-a} &amp; \\text{ if } x\\in [a,b]\\\\0 &amp; \\text{ if } x\\notin [a,b]\\end{cases}\\) \\(E(f(X))=\\int f(x)\\phi(x)dx=\\int_a^b h(x)dx\\) Also, \\(f(x)\\phi(x)= f(x)\\boldsymbol 1_{[a,b]}\\frac{1}{b-a}=f(x)\\frac{1}{b-a} \\forall x \\in [a,b]\\) \\(\\implies f(x)=h(x)(b-a)\\) So, if \\(X\\sim U(a,b)\\), then \\(\\int_a^b h(x)dx=E((b-a)h(X))\\approx \\frac1N \\sum_{j=1}^{N}(b-a)h(X_j)\\), where \\(X_j\\sim U(a,b)\\) are i.i.d. # aim: integrate h(x)=x^2 from 2 to 3 N &lt;- 1e6 X &lt;- runif(N, 2, 3) f.of.X &lt;- X^2*(3-2) mean(f.of.X) ## [1] 6.330978 Analytic solution: \\(\\int_{2}^{3}x^2dx=\\frac13x^3|^{3}_{x=2}=\\frac133^3-\\frac132^3\\\\=\\frac1327-\\frac138=9-\\frac83=\\frac{19}{3}\\) 19/3 ## [1] 6.333333 4.2 Monte Carlo estimates 4.2.1 Computing Monte Carlo estimates Recall: \\(Z_N^{MC}=\\frac1N\\sum_{j=1}^{N}f(X_j)\\approx E(f(X))\\), where \\(X_j\\) are i.i.d. copies of \\(X\\) need to generate the \\(X_j\\) (see chapter 1 in the book) need to apply \\(f\\) to get \\(f(X_j)\\) need to compute the average, a lot of memory is needed to store \\(X_1,...,X_N\\) To save memory, we could sum up the values, as we generate them: s &lt;- 0 for j=1,...,N: generate X_j s &lt;- s + f(X_j) result: \\(s/N\\) The downside is that the for loop in R is relatively slow, so there’s a trade-off. Use the for loop if you run out of memory. We will also need to compute the sample variance of the \\(f(X_j):\\) \\(s^2_{f(X_j)}=\\frac{1}{N-1}\\sum_{j=1}^{N}(f(X_j)-\\bar{f(X)})^2\\\\=\\frac{1}{N-1}\\sum_{j=1}^{N}f(X_j)^2-\\frac{N}{N-1}(\\frac1N\\sum_{j=1}^{N}f(X_j))^2\\), where \\(\\bar{f(X)}=\\frac1N\\sum_{j=1}^{N}f(X_j)\\) s &lt;- 0 t &lt;- 0 for j=1,..,N: generate X_j s &lt;- s + f(X_j) t &lt;- t + (f(X_j))^2 Then \\(Z_N^{MC}=s/N, s^2=\\frac{t}{N-1}-\\frac{N}{N-1}(Z_N^{MC})^2\\) 4.2.2 Monte Carlo error and Choice of sample size Recall: \\(Z_N^{MC}=\\frac1N\\sum_{j=1}^{N}f(X_j)\\approx E(f(X))\\) \\(bias(Z_N^{MC})=E(Z_N^{MC})-E(f(X))\\) \\(std. error(Z_N^{MC})=std.dev(Z_N^{MC})=\\sqrt{Var(Z_N^{MC})}\\) \\(MSE(Z_N^{MC})=E(Z_N^{MC}-E[f(X)]^2)=Var(Z_N^{MC})-bias(Z_N^{MC})^2\\) \\(E(Z_N^{MC})=E(\\frac1N\\sum_{j=1}^{N}f(X_j))=\\frac1N\\sum_{j=1}^{N} E(f(X_j))=\\frac1N\\sum_{j=1}^{N} E(f(X))=\\frac N N E(f(X))=E(f(X))\\) In the above, we use the fact that \\(X_j\\) are i.i.d copies of \\(X\\) Hence, we get \\(bias(Z_N^{MC})=0\\), and \\(MSE(Z_N^{MC})=Var(Z_N^{MC})=std. error(Z_N^{MC})^2\\) \\(Var(Z_N^{MC})=Var(\\frac1N \\sum_{j=1}^{N}f(X_j))=\\\\\\frac{1}{N^2}\\sum_{j=1}^{N}Var(f(X_j))=\\frac{1}{N^2}\\sum_{j=1}^{N}Var(f(X))=\\frac1N Var(f(X))\\) Hence, we get \\(MSE(Z_N^{MC})=Var(Z_N^{MC})=\\frac1N Var(f(X))\\) \\(RMSE(Z_N^{MC})=\\sqrt{\\frac{Var(f(X))}{N}}=\\frac{std.error(Z_N^{MC})}{\\sqrt{N}}\\) To achieve \\(RMSE(Z_N^{MC})\\le \\epsilon\\), we need \\(\\epsilon\\ge RMSE(Z_N^{MC})\\implies \\epsilon^2\\ge MSE(Z_N^{MC})=\\frac1N Var(f(X))\\\\\\implies N\\ge \\frac{Var(f(X))}{\\epsilon^2}\\) We can estimate \\(Var(f(X))\\) from data, using the sample variance of the \\(f(X_j)\\). 4.2.3 Refined error bounds \\(e_N^{MC}=Z_N^{MC}-E(f(X))\\) Recall Law of Large Numbers (LLN), \\[\\lim_{N\\to \\infty}\\frac1N\\sum_{j=1}^{N}f(X_j)=E(f(X))\\\\ \\implies \\lim_{N\\to \\infty}Z_N^{MC}=E(f(X))\\\\ \\implies \\lim_{N\\to \\infty}e_N^{MC}=\\lim_{N\\to \\infty}(Z_N^{MC}-E[f(X)])=0\\] Note that this is true even if \\(Var(f(X))=\\infty\\) Recall Central Limit Theorem (CLT), \\[\\frac{\\sqrt{N}e_N^{MC}}{\\sigma}=\\frac{\\sqrt N}{\\sigma}(Z_N^{MC}-E[f(X)])\\\\= \\frac{\\sqrt N}{\\sigma}(\\frac1N \\sum_{j=1}^{N}f(X_j)-\\frac1N\\sum_{j=1}^{N}E[f(X)])\\\\ =\\frac{1}{\\sqrt N}\\sum_{j=1}^{N}\\frac{f(X_j)-E[f(X)]}{\\sigma}\\to^d N(0,1)\\text{ as } N\\to \\infty\\] where \\(\\sigma=\\sqrt{Var(f(X_j))}=\\sqrt{Var(f(X))}\\) So, for large N (approximately): \\[\\frac{\\sqrt N}{\\sigma}e_N^{MC}\\sim N(0,1)\\\\ \\implies e_N^{MC}\\sim N(0, \\frac{\\sigma^2}{N})=N(0, (\\frac{\\sigma}{\\sqrt N})^2)\\] Hence, we can have: \\(P(|e_N^{MC}|&gt;1.96\\frac{\\sigma}{\\sqrt N})= 5\\%\\) or \\(P(E[f(X)]\\in [Z_N^{MC}-1.96\\frac{\\sigma}{\\sqrt N},Z_N^{MC}+1.96\\frac{\\sigma}{\\sqrt N}])=95\\%\\) 4.3 Variance reduction methods 4.3.1 Importance sampling 4.3.2 Antithetic variables 4.3.3 Control variates "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
